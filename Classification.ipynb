{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project4 Artificial intelligence\n",
    "### Fatemeh Haghighi  810195385"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import mode\n",
    "import numpy as np\n",
    "import operator\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 : Implement Decision tree with sklearn library \n",
    "In this part first we implement dicision tree with \"tree.DecisionTreeClassifier()\" function then, we separate current data in two parts, X_train and X_test. one for learning the tree (by fitting) and the other for testing it. after making the tree we fit the train data into tree and predict the lable of test data based on learned information.\n",
    "after that we could caculate the accuracy of our model by knowing the exact lables of test data in comprasion of predicted lables.\n",
    "### Split data into test part and train part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(data, test_size=0.2, random_state=12347, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dicision tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split target and featur parts in train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = X_train.target\n",
    "train_features = X_train.drop(['target'], axis = 1)\n",
    "\n",
    "test_label = X_test.target\n",
    "test_features = X_test.drop(['target'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit train features in dicision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf.fit(train_features, train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of learned tree based on train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy in train data is  1.0\n"
     ]
    }
   ],
   "source": [
    "train_prediction = clf.predict(train_features)\n",
    "train_accuracy = accuracy_score(train_label, train_prediction)\n",
    "print(\"accuracy in train data is \", train_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of learned tree based on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy in test data is  0.7868852459016393\n"
     ]
    }
   ],
   "source": [
    "test_prediction = clf.predict(test_features)\n",
    "test_accuracy = accuracy_score(test_label, test_prediction)\n",
    "print(\"accuracy in test data is \", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get mean of test data accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7591475409836065\n",
      "0.024857216552285635\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for counter in range(1000):\n",
    "    clf = clf.fit(train_features, train_label)\n",
    "    test_prediction = clf.predict(test_features)\n",
    "    result.append(accuracy_score(test_label, test_prediction))\n",
    "result = np.array(result)\n",
    "mean = result.mean()\n",
    "std = result.std()\n",
    "print(mean)\n",
    "print(std)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 : Implement random forest\n",
    "In this part, first we partition our test data in 5 groups that each has 150 elements.\n",
    "then we implement bagging class that has some methods.every object from this class patitions data into some group(based on input from constructor) and made tree, that every group sample use for learn a tree, it means that we get number of trees from input and partition data based on that too.then we predict test data label on each tree and get mode of the result. in the other words, for label of each row we accept the value that has the most frequent  among all 5 group for that row . after that we can calculate accuracy with axact labels and predicted labels.as you know most significant bagging's methods are : fit and predict as their functionality said briefly.\n",
    "after that we calculate accuracy of our model by fiting train data with less features, it means for each feature, we calculate accuracy of model based on data that does not contain that feature. as a result we could find a feature that by ignoring that, we found accuracy that has minimum difference with original one on dicision tree. \n",
    "then we made a dicision tree that fites with train data containing 5 random feature that implemented below.\n",
    "after all we implement random forest class, that each object of this class make numbers of tree(based on tree input number) that train data for each class, has random number of features(more than zero and less than maximum number of columns) and fixed number of row that selected randomly from original train data based on element number from input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.1 : Partition data into 5 group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_data(group_number, element_number):\n",
    "    group_list = []\n",
    "    for counter in range(group_number):\n",
    "        sample_data = data.sample(n = element_number, replace = True) \n",
    "        group_list.append(sample_data)\n",
    "    return group_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_list = get_group_data(5, 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2: Implement bagging model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bagging:\n",
    "    def __init__(self, tree_number, element_each_tree):\n",
    "        self.train_data = None\n",
    "        self.tree_number = tree_number\n",
    "        self.element_each_tree = element_each_tree\n",
    "        self.train_data_groups = []\n",
    "        self.train_featurs_groups = []\n",
    "        self.train_lable_groups = []\n",
    "        self.dicision_trees = []\n",
    "        self.create_dicision_trees()\n",
    "\n",
    "        \n",
    "    def get_group_data(self):\n",
    "\n",
    "        for counter in range(self.tree_number):\n",
    "            sample_data = self.train_data.sample(n = self.element_each_tree, replace = True)\n",
    "            self.train_data_groups.append(sample_data)\n",
    "    \n",
    "    def separate_labels(self):\n",
    "        for df in self.train_data_groups:\n",
    "            self.train_lable_groups.append(df.target)\n",
    "            self.train_featurs_groups.append(df.drop(['target'], axis = 1))\n",
    "                \n",
    "    def create_dicision_trees(self):\n",
    "        for counter in range(self.tree_number):\n",
    "            clf = tree.DecisionTreeClassifier()\n",
    "            self.dicision_trees.append(clf)\n",
    "            \n",
    "    def fit(self, train_df):\n",
    "        self.train_data = train_df\n",
    "\n",
    "        self.get_group_data()\n",
    "        self.separate_labels()\n",
    "        \n",
    "        for i in range(self.tree_number):\n",
    "            self.dicision_trees[i].fit(self.train_featurs_groups[i], self.train_lable_groups[i])\n",
    "    \n",
    "    def predict(self, test_data):\n",
    "        test_data_label = test_data.target\n",
    "        test_data_features = test_data.drop(['target'], axis = 1)\n",
    "\n",
    "        predictions = []\n",
    "        for tree in self.dicision_trees:\n",
    "            predictions.append(tree.predict(test_data_features))\n",
    "        result = mode(predictions, axis = 0)[0][0]\n",
    "        return result\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of train data on learned tree is :  0.8032786885245902\n"
     ]
    }
   ],
   "source": [
    "B = Bagging(5, 150)\n",
    "B.fit(X_train)\n",
    "result = B.predict(X_test)\n",
    "bagging_train_accuracy = accuracy_score(test_label, result)\n",
    "print(\"Accuracy of train data on learned tree is : \", bagging_train_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of test data on learned tree is :  0.819672131147541\n"
     ]
    }
   ],
   "source": [
    "B.fit(X_train)\n",
    "test_predict_result = B.predict(X_test)\n",
    "bagging_test_accuracy = accuracy_score(test_label, test_predict_result)\n",
    "print(\"Accuracy of test data on learned tree is : \", bagging_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get mean of bagging accuracy over test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7947540983606558\n",
      "0.020798362658562254\n"
     ]
    }
   ],
   "source": [
    "r = []\n",
    "for counter in range(100):\n",
    "    B.fit(X_train)\n",
    "    test_predict_result = B.predict(X_test)\n",
    "    r.append(accuracy_score(test_label, test_predict_result))\n",
    "r = np.array(r)\n",
    "mean = r.mean()\n",
    "std = r.std()\n",
    "print(mean)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.3 : Remove each feature and calculate accuracy of new datafarame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = {}\n",
    "def remove_feature(train_label, train_features, test_label, test_features):\n",
    "    tree_rmv_features = tree.DecisionTreeClassifier()\n",
    "    for col in train_features.columns:\n",
    "        res = []\n",
    "        for i in range(500):\n",
    "            train_sample = train_features.drop([col], axis = 1)\n",
    "            test_sample = test_features.drop([col], axis = 1)\n",
    "            tree_rmv_features.fit(train_sample, train_label)\n",
    "            p = tree_rmv_features.predict(test_sample)\n",
    "            res.append(accuracy_score(test_label, p))\n",
    "        res = np.array(res)\n",
    "        accs[col] = res.mean()\n",
    "    return accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': 0.822032786885246,\n",
       " 'ca': 0.7731147540983605,\n",
       " 'chol': 0.7290819672131147,\n",
       " 'cp': 0.8325573770491804,\n",
       " 'exang': 0.7453770491803278,\n",
       " 'fbs': 0.8092459016393443,\n",
       " 'oldpeak': 0.7315409836065574,\n",
       " 'restecg': 0.7531475409836064,\n",
       " 'sex': 0.7562950819672131,\n",
       " 'slope': 0.7582295081967212,\n",
       " 'thal': 0.7407540983606556,\n",
       " 'thalach': 0.786655737704918,\n",
       " 'trestbps': 0.78127868852459}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs = remove_feature(train_label, train_features, test_label, test_features)\n",
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deletion of feature :  cp cause accuracy that has minimum diference with original one \n"
     ]
    }
   ],
   "source": [
    "print(\"deletion of feature : \", max(accs.items(), key=operator.itemgetter(1))[0], \\\n",
    "      \"cause accuracy that has minimum diference with original one \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.4 : Make dicision tree based on 5 random features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7147540983606557\n",
      "0.07184368950423008\n"
     ]
    }
   ],
   "source": [
    "res1 = []\n",
    "for i in range(3000):\n",
    "    test_f = copy.deepcopy(test_features)\n",
    "    train_f = copy.deepcopy(train_features)\n",
    "    \n",
    "    columns = np.array(train_f.columns)\n",
    "    selected = np.random.choice(columns , size = 5 , replace = False)\n",
    "    \n",
    "    train_f_sel = train_f[selected]\n",
    "    test_f_sel = test_f[selected]\n",
    "            \n",
    "    small_tree = tree.DecisionTreeClassifier()\n",
    "    small_tree.fit(train_f_sel, train_label)\n",
    "    p = small_tree.predict(test_f_sel)\n",
    "    res1.append(accuracy_score(test_label, p))\n",
    "res1 = np.array(res1)\n",
    "print(res1.mean())\n",
    "print(res1.std())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.5 : Build random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Random_forest():\n",
    "    def __init__(self, tree_number, element_number):\n",
    "        self.tree_number = tree_number\n",
    "        self.element_number = element_number\n",
    "        self.trees_columns = []\n",
    "        self.trees = []\n",
    "        \n",
    "    def fit(self, train_data):\n",
    "        for i in range(self.tree_number):\n",
    "            self.trees.append(tree.DecisionTreeClassifier())            \n",
    "#           select some rows for train data of each tree        \n",
    "            train_sampled = train_data.sample(n = self.element_number, replace = True)\n",
    "#           separate target and features part of original data\n",
    "            train_label = train_sampled.target\n",
    "            train_features = train_sampled.drop(['target'], axis = 1)\n",
    "#           select columns from features set\n",
    "            columns = np.array(train_features.columns)\n",
    "            features_number = np.random.randint(1 , len(train_sampled.columns) ) \n",
    "            self.trees_columns.append(np.random.choice(columns , size = features_number , replace = False))\n",
    "#       fit the trees\n",
    "        for i in range(self.tree_number):\n",
    "            train_f_sel = train_sampled[self.trees_columns[i]] \n",
    "            self.trees[i].fit(train_f_sel, train_label)\n",
    "            \n",
    "    def predict(self, test_data):\n",
    "        test_data_label = test_data.target\n",
    "        test_data_features = test_data.drop(['target'], axis = 1)\n",
    "        predictions = []\n",
    "        for i in range(self.tree_number):\n",
    "            predictions.append(self.trees[i].predict(test_data_features[self.trees_columns[i]]))\n",
    "        result = mode(predictions, axis = 0)[0][0]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.819672131147541\n"
     ]
    }
   ],
   "source": [
    "f = Random_forest(40, 150) \n",
    "f.fit(X_train)\n",
    "pred = f.predict(X_test)\n",
    "print(accuracy_score(pred , test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get mean of random forest accuracy based  on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8327868852459019\n",
      "0.039684322743967874\n"
     ]
    }
   ],
   "source": [
    "r = []\n",
    "for counter in range(100):\n",
    "    f.fit(X_train)\n",
    "    test_predict_result = f.predict(X_test)\n",
    "    r.append(accuracy_score(test_label, test_predict_result))\n",
    "r = np.array(r)\n",
    "mean = r.mean()\n",
    "std = r.std()\n",
    "print(mean)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first question:\n",
    "Bootstrap aggregating, also called bagging , is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach.\n",
    "\n",
    "In the other words, When training, each tree in a random forest learns from a random sample of the data points. The samples are drawn with replacement, known as bootstrapping, which means that some samples will be used multiple times in a single tree.\n",
    "bootstraping increase standard deviation and variance of accuracy of a model because of randomness forced to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second question:\n",
    "-Over-fitting is the phenomenon in which the learning system tightly fits the given training data so much that it would be inaccurate in predicting the outcomes of the untrained data. In decision trees, over-fitting occurs when the tree is designed so as to perfectly fit all samples in the training data set.\n",
    "-dicision tree is sensitive to over fitting because dicision tree have no idea about train data and learn data completely by fitting so its sensitive to overfitting.\n",
    "-in bagging we select some rows from original data randomly, so we do not have over fitting on whole data with bagging but we have over fitting on part of data that selected randomly. but in bagging probablity of reaching to wrong result is less that correct result because we get sample from original data randomly in number of trees and accept most frequent answer for each row. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third question:\n",
    "Random forest, like its name implies, consists of a large number of individual decision trees that operate as an ensemble. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model’s prediction.\n",
    "Bootstrap aggregating, also called bagging , is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach.\n",
    "\n",
    "The fundamental difference is that in Random forests, only a subset of features are selected at random out of the total and the best split feature from the subset is used to split each node in a tree, unlike in bagging where all features are considered for splitting a node.\n",
    "In a more widesense comparison Random Forest uses Bagging to overcome overfiting in decision tree.\n",
    "on the other words, random forest uses bagging to solve over fiting problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth question:\n",
    "accuracy of part 1: 0.7591 <br>\n",
    "accuracy of part 2.2: 0.7947 <br>\n",
    "accuracy of part 2.5: 0.8327 <br> \n",
    "accuracies are in average case. <br>\n",
    "from these numbers we can understood that having some(more that one) tree or model helps us to achieve much better result in prediction, in comparition of having single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
